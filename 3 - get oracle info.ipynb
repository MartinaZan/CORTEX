{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that can be used to plot the GCN embeddings (first 2 components) and compute the accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from utils_martina.my_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set patient and record\n",
    "\n",
    "# patient_id = \"chb04\"\n",
    "# record_id = \"28\"\n",
    "\n",
    "patient_id = \"PN00\"\n",
    "record_id = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\logs\\\\\"\n",
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "embeddings_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\embeddings\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19976-Martina\n"
     ]
    }
   ],
   "source": [
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logs\n",
    "with open(logs_path + file_name + '.info', \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Load eval_manager\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zanno\\miniconda3\\envs\\GRETEL_2\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\zanno\\miniconda3\\envs\\GRETEL_2\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "# Get data_instance.id, data_instance.label, prediction, node embeddings\n",
    "all_embeddings = get_embeddings_and_outputs_all(eval_manager)\n",
    "test_embeddings = get_embeddings_and_outputs_test(eval_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if patient_id != \"\":\n",
    "    _, dict_record = extract_time_and_record(content)\n",
    "    id_graphs = [id for id, value in dict_record.items() if value == f\"{patient_id}_{record_id}\"]\n",
    "    test_embeddings = [item for item in test_embeddings if str(item['data_instance_id']) in id_graphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data):\n",
    "    \"\"\"\n",
    "    Calculates the metrics to be shown in the plot\n",
    "    \"\"\"\n",
    "    y_true = [item['data_instance_label'] for item in data]\n",
    "    y_pred = [item['prediction'] for item in data]\n",
    "    \n",
    "    # cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        # \"Confusion Matrix\": cm,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Accuracy\": accuracy\n",
    "    }\n",
    "\n",
    "def plot_embeddings_with_metrics(data, title, ax):\n",
    "    \"\"\"\n",
    "    Function to plot the embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    # Collect x and y:\n",
    "\n",
    "    # Label = 0, Preditcion = 0\n",
    "    x_0_correct = [d['embeddings'][0][0] for d in data if d['data_instance_label'] == 0 and d['prediction'] == 0]\n",
    "    y_0_correct = [d['embeddings'][0][1] for d in data if d['data_instance_label'] == 0 and d['prediction'] == 0]\n",
    "\n",
    "    # Label = 1, Preditcion = 1\n",
    "    x_1_correct = [d['embeddings'][0][0] for d in data if d['data_instance_label'] == 1 and d['prediction'] == 1]\n",
    "    y_1_correct = [d['embeddings'][0][1] for d in data if d['data_instance_label'] == 1 and d['prediction'] == 1]\n",
    "    \n",
    "    # Label = 0, Preditcion = 1\n",
    "    x_0_incorrect = [d['embeddings'][0][0] for d in data if d['data_instance_label'] == 0 and d['prediction'] == 1]\n",
    "    y_0_incorrect = [d['embeddings'][0][1] for d in data if d['data_instance_label'] == 0 and d['prediction'] == 1]\n",
    "\n",
    "    # Label = 1, Preditcion = 0\n",
    "    x_1_incorrect = [d['embeddings'][0][0] for d in data if d['data_instance_label'] == 1 and d['prediction'] == 0]\n",
    "    y_1_incorrect = [d['embeddings'][0][1] for d in data if d['data_instance_label'] == 1 and d['prediction'] == 0]\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    # Compute metrics\n",
    "    tn = len(x_0_correct)\n",
    "    tp = len(x_1_correct)\n",
    "    fp = len(x_0_incorrect)\n",
    "    fn = len(x_1_incorrect)\n",
    "    \n",
    "    metrics = calculate_metrics(data)\n",
    "    # metrics.update({'False positive rate': fn/(fn+tn)})\n",
    "\n",
    "    for i, (metric, value) in enumerate(metrics.items()):\n",
    "        ax.annotate(f\"{metric}: {value:.3f}\" + \"\\n\"*i, fontsize=10, xy=(1.025, 0.35), xycoords='axes fraction')\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    # Plot embeddings\n",
    "    ax.scatter(x_0_correct, y_0_correct, color='blue', label=f'Label 0 (classified as 0): {tn}', alpha=0.15, marker='o', s=30)\n",
    "    ax.scatter(x_0_incorrect, y_0_incorrect, color='midnightblue', label=f'Label 0 (classified as 1): {fp}', alpha=0.75, marker='x', s=8)\n",
    "    \n",
    "    ax.scatter(x_1_correct, y_1_correct, color='red', label=f'Label 1 (classified as 1): {tp}', alpha=0.15, marker='o', s=30)\n",
    "    ax.scatter(x_1_incorrect, y_1_incorrect, color='darkred', label=f'Label 1 (classified as 0): {fn}', alpha=0.75, marker='x', s=8)\n",
    "\n",
    "    #####################################################################################################################\n",
    "\n",
    "    # Plot\n",
    "    ax.set_xlabel(r'$x_0$')\n",
    "    ax.set_ylabel(r'$x_1$')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(data):\n",
    "    # Label = 0, Preditcion = 0 \n",
    "    tn = len([d for d in data if d['data_instance_label'] == 0 and d['prediction'] == 0])\n",
    "\n",
    "    # Label = 1, Preditcion = 1\n",
    "    tp = len([d for d in data if d['data_instance_label'] == 1 and d['prediction'] == 1])\n",
    "    \n",
    "    # Label = 0, Preditcion = 1\n",
    "    fp = len([d for d in data if d['data_instance_label'] == 0 and d['prediction'] == 1])\n",
    "\n",
    "    # Label = 1, Preditcion = 0\n",
    "    fn = len([d for d in data if d['data_instance_label'] == 1 and d['prediction'] == 0])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(data)\n",
    "\n",
    "    for i, (metric, value) in enumerate(metrics.items()):\n",
    "        print(f\"       {metric}: {value:.3f}\")\n",
    "\n",
    "    print(f\"\\n       Label 0 (classified as 0): {tn}\")\n",
    "    print(f\"       Label 0 (classified as 1): {fp}\")\n",
    "    print(f\"       Label 1 (classified as 1): {tp}\")\n",
    "    print(f\"       Label 1 (classified as 0): {fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-->> Train set\n",
      "       Precision: 0.844\n",
      "       Recall: 0.877\n",
      "       F1 Score: 0.861\n",
      "       Accuracy: 0.851\n",
      "\n",
      "       Label 0 (classified as 0): 306\n",
      "       Label 0 (classified as 1): 66\n",
      "       Label 1 (classified as 1): 358\n",
      "       Label 1 (classified as 0): 50\n",
      "\n",
      "-->> Test set (PN00_2)\n",
      "       Precision: 0.818\n",
      "       Recall: 1.000\n",
      "       F1 Score: 0.900\n",
      "       Accuracy: 0.905\n",
      "\n",
      "       Label 0 (classified as 0): 10\n",
      "       Label 0 (classified as 1): 2\n",
      "       Label 1 (classified as 1): 9\n",
      "       Label 1 (classified as 0): 0\n"
     ]
    }
   ],
   "source": [
    "if eval_manager._evaluators[0]._oracle.name[:3] == 'GCN':\n",
    "    fig, ax = plt.subplots(2, figsize=(7,7))\n",
    "    ax[0] = plot_embeddings_with_metrics(all_embeddings, f'Node embeddings $(x_0,x_1)$ - train set', ax=ax[0])\n",
    "    ax[1] = plot_embeddings_with_metrics(test_embeddings, f\"Node embeddings $(x_0,x_1)$ - test set{' (' + patient_id + '_' + record_id + ')' if patient_id != '' else ''}\", ax=ax[1])\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{embeddings_path}{file_name.split('-')[0]}_{patient_id}_{record_id}.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"-->> Train set\")\n",
    "    print_metrics(all_embeddings)\n",
    "\n",
    "    print(f\"\\n-->> Test set{' (' + patient_id + '_' + record_id + ')' if patient_id != '' else ''}\")\n",
    "    print_metrics(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _______________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last full run: 27/03/2025, ore 17:13\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last full run:\", now.strftime(\"%d/%m/%Y, ore %H:%M\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
