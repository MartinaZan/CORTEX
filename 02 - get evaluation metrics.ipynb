{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils.utils import *\n",
    "from src.dataset.instances.graph import GraphInstance\n",
    "from src.evaluation.evaluation_metric_runtime import RuntimeMetric\n",
    "from src.evaluation.evaluation_metric_correctness import CorrectnessMetric\n",
    "from src.evaluation.evaluation_metric_fidelity import FidelityMetric\n",
    "from src.evaluation.evaluation_metric_implausibility import ImplausibilityMetric\n",
    "from src.evaluation.evaluation_metric_dissimilarity import M_dissim_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19708-Martina\n"
     ]
    }
   ],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "file_name = \"19708-Martina\"\n",
    "print(file_name)\n",
    "\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oracle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_record  accuracy  f1_score  recall  precision\n",
      "0            ALL    0.9402    0.9434  0.9470     0.9398\n",
      "1       chb03_01    0.9336    0.9363  0.9124     0.9615\n",
      "2       chb03_02    0.9550    0.9580  0.9779     0.9388\n",
      "3       chb03_03    0.9554    0.9593  0.9709     0.9480\n",
      "4       chb03_04    0.9708    0.9720  0.9732     0.9708\n",
      "5       chb03_34    0.9052    0.9049  0.8686     0.9444\n",
      "6       chb03_35    0.9376    0.9431  0.9854     0.9042\n",
      "7       chb03_36    0.9246    0.9281  0.9407     0.9159\n"
     ]
    }
   ],
   "source": [
    "def get_oracle_metrics(eval_manager):\n",
    "    instances = eval_manager.evaluators[0].dataset.instances\n",
    "    oracle = eval_manager.evaluators[0]._oracle\n",
    "\n",
    "    grouped = {}\n",
    "    for inst in instances:\n",
    "        key = f\"{inst.patient_id}_{inst.record_id}\"\n",
    "        grouped.setdefault(key, []).append(inst)\n",
    "\n",
    "    rows = []\n",
    "    for pr, inst_list in grouped.items():\n",
    "        y_true = [i.label for i in inst_list]\n",
    "        y_pred = [oracle.predict(i) for i in inst_list]\n",
    "        rows.append({\n",
    "            \"patient_record\": pr,\n",
    "            \"accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"f1_score\": round(f1_score(y_true, y_pred),4),\n",
    "            \"recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"precision\": round(precision_score(y_true, y_pred),4)\n",
    "        })\n",
    "\n",
    "    y_true_all = [i.label for i in instances]\n",
    "    y_pred_all = [oracle.predict(i) for i in instances]\n",
    "    global_row = {\n",
    "        \"patient_record\": \"ALL\",\n",
    "        \"accuracy\": round(accuracy_score(y_true_all, y_pred_all),4),\n",
    "        \"f1_score\": round(f1_score(y_true_all, y_pred_all),4),\n",
    "        \"recall\": round(recall_score(y_true_all, y_pred_all),4),\n",
    "        \"precision\": round(precision_score(y_true_all, y_pred_all),4)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([global_row] + rows)\n",
    "\n",
    "oracle_metrics = get_oracle_metrics(eval_manager)\n",
    "print(oracle_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top $k$ counterfactuals and explainer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_keys = ['ID', 'Runtime', 'Correctness', 'Fidelity', 'Implausibility', 'Dissimilarity', 'Accuracy']\n",
    "metrics_keys = ['ID', 'Correctness', 'Fidelity', 'Implausibility', 'Dissimilarity', 'Accuracy']\n",
    "metrics = {k: {} for k in metrics_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) DataDrivenBidirectionalSearchExplainer\n",
      "(1) ObliviousBidirectionalSearchExplainer\n",
      "(2) TemporalDCESExplainer: (0.7, 0.2, 0.1)\n",
      "(3) TemporalDCESExplainerNoStability: (1, 0, X)\n",
      "(4) GNNMOExp\n"
     ]
    }
   ],
   "source": [
    "for i, evaluator in enumerate(eval_manager._evaluators):\n",
    "    explainer = evaluator._explainer\n",
    "    name = explainer.name.split('-')[0]\n",
    "    if \"Temporal\" in name:\n",
    "        if 'NoStability' not in name:\n",
    "            print(f\"({i}) {name}: ({explainer.alpha}, {explainer.beta}, {explainer.gamma})\")\n",
    "        else:\n",
    "            print(f\"({i}) {name}: ({explainer.alpha}, {explainer.beta}, X)\")\n",
    "    else:\n",
    "        print(f\"({i}) {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(eval_manager._evaluators)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluator 0: 100%|██████████| 2870/2870 [7:14:34<00:00,  9.09s/it]  \n",
      "Evaluator 1: 100%|██████████| 2870/2870 [3:32:55<00:00,  4.45s/it]  \n",
      "Evaluator 2: 100%|██████████| 2870/2870 [1:15:50<00:00,  1.59s/it]\n",
      "Evaluator 3: 100%|██████████| 2870/2870 [3:35:40<00:00,  4.51s/it]  \n",
      "Evaluator 4: 100%|██████████| 2870/2870 [04:37<00:00, 10.33it/s]\n"
     ]
    }
   ],
   "source": [
    "correctness_metric = CorrectnessMetric()\n",
    "fidelity_metric = FidelityMetric()\n",
    "implausibility_metric = ImplausibilityMetric()\n",
    "dissimilarity_metric = M_dissim_metric()\n",
    "# runtime_metric = RuntimeMetric()\n",
    "\n",
    "all_cf_dicts = {}\n",
    "\n",
    "for i in indices:\n",
    "    # Initialize empty lists for each metric for this evaluator\n",
    "    for key in metrics_keys:\n",
    "        metrics[key][i] = []\n",
    "\n",
    "    cf_dict = {}\n",
    "\n",
    "    evaluator = eval_manager._evaluators[i]\n",
    "    oracle = evaluator._oracle\n",
    "    explainer = evaluator._explainer\n",
    "    dataset = evaluator.dataset.instances\n",
    "\n",
    "    # Select only correctly predicted positive instances\n",
    "    list_instances = [\n",
    "        inst for inst in dataset\n",
    "        if inst.label == 1\n",
    "    ]\n",
    "\n",
    "    # list_instances = list_instances[:10]\n",
    "\n",
    "    # Check if the explainer's explain method takes two arguments (i.e., has return_list=True)\n",
    "    has_return_list = len(inspect.signature(explainer.explain).parameters) == 2\n",
    "\n",
    "    for instance in tqdm(list_instances, desc=f\"Evaluator {i}\"):\n",
    "        # Generate counterfactual for the instance\n",
    "        result = explainer.explain(instance, return_list=True) if has_return_list else explainer.explain(instance)\n",
    "        cf_dict[instance.id] = result\n",
    "\n",
    "        # Extract the counterfactual instance\n",
    "        counterfactual = result if isinstance(result, GraphInstance) else result[0][1]\n",
    "\n",
    "        # Record info and metrics\n",
    "        metrics['ID'][i].append(instance.id)\n",
    "        # metrics['Runtime'][i].append(runtime_metric.evaluate(instance, counterfactual, oracle, explainer)[0])\n",
    "        metrics['Correctness'][i].append(correctness_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Fidelity'][i].append(fidelity_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Implausibility'][i].append(implausibility_metric.evaluate(instance, counterfactual, dataset=dataset))\n",
    "        metrics['Dissimilarity'][i].append(dissimilarity_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Accuracy'][i].append(oracle.predict(instance) == 1)\n",
    "\n",
    "    all_cf_dicts[i] = cf_dict\n",
    "\n",
    "with open(f\"output/cf_dict/cf_dict_{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_cf_dicts, f)\n",
    "\n",
    "with open(f\"output/cf_dict/metrics_{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"output/cf_dict/cf_dict_{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_cf_dicts, f)\n",
    "\n",
    "with open(f\"output/cf_dict/metrics_{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se serve, la runtime la faccio a posteriori in futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainer metrics (all patients, only top counterfactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Correctness  Fidelity  Implausibility  Dissimilarity\n",
      "Evaluator                                                      \n",
      "0               0.1160    0.0854          0.2392         0.2392\n",
      "1               0.2819    0.2226          0.1547         0.1547\n",
      "2               0.9470    0.9470          0.0000         0.9730\n",
      "3               0.9997    0.8941          0.0000         0.9486\n",
      "4               0.1889    0.1087             NaN            NaN\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "with open(f\"output/cf_dict/metrics_{file_name}.pkl\", 'rb') as f:\n",
    "    metrics = pickle.load(f)\n",
    "\n",
    "for eval_id in metrics['ID'].keys():\n",
    "    row = {\"Evaluator\": eval_id}\n",
    "    acc = np.array(metrics['Accuracy'][eval_id])\n",
    "\n",
    "    for metric in metrics_keys[1:]:\n",
    "        vals = np.array(metrics[metric][eval_id])\n",
    "\n",
    "        # Implausibility and Dissimilarity are only computed for well-predicted instances\n",
    "        if metric in ['Implausibility', 'Dissimilarity']:\n",
    "            filtered_vals = vals[acc == True]\n",
    "            mean_val = np.mean(filtered_vals) if filtered_vals.size > 0 else np.nan\n",
    "        else:\n",
    "            mean_val = np.mean(vals) if vals.size > 0 else np.nan\n",
    "        row[metric] = round(mean_val, 4) if not np.isnan(mean_val) else np.nan\n",
    "        \n",
    "    rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"Evaluator\")\n",
    "print(summary_df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation study (single patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record: chb03_01\n",
      "\n",
      "        Parameters         M_dissim        sqrt(M_time)         M_instab\n",
      "0  (0.7, 0.2, 0.1)  0.4238 ± 0.0084   81.8231 ± 32.3935  0.4868 ± 0.2077\n",
      "1        (1, 0, X)  0.4015 ± 0.0112  171.3438 ± 86.3431        nan ± nan\n"
     ]
    }
   ],
   "source": [
    "patient_id = \"chb03\"\n",
    "record_id = \"01\"\n",
    "\n",
    "# Carica tutti i cf_dict da file unico\n",
    "with open(f\"output/cf_dict/cf_dict_{file_name}.pkl\", \"rb\") as f:\n",
    "    all_cf_dicts = pickle.load(f)\n",
    "\n",
    "# Crea dizionario di set con gli ID corretti per ogni evaluator\n",
    "accurate_ids = {\n",
    "    i: set(\n",
    "        id_ for id_, acc in zip(metrics['ID'][i], metrics['Accuracy'][i]) if acc\n",
    "    )\n",
    "    for i in metrics['ID'].keys()\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, evaluator in enumerate(eval_manager._evaluators):\n",
    "    explainer = evaluator._explainer\n",
    "\n",
    "    if 'Temporal' not in explainer.name:\n",
    "        continue\n",
    "\n",
    "    # Prendi il cf_dict corrispondente a questo evaluator\n",
    "    cf_dict = all_cf_dicts[i]\n",
    "\n",
    "    m_d, m_t, m_i = [], [], []\n",
    "\n",
    "    for instance in evaluator.dataset.instances:\n",
    "        if instance.patient_id != patient_id or instance.record_id != record_id:\n",
    "            continue\n",
    "\n",
    "        if instance.id not in accurate_ids.get(i, set()):\n",
    "            continue\n",
    "\n",
    "        d_vals, t_vals, i_vals = [], [], []\n",
    "\n",
    "        for cf in cf_dict[instance.id]:\n",
    "            d, t, instab = explainer.compute_metric_components(instance, cf[1])\n",
    "            d_vals.append(d)\n",
    "            t_vals.append(np.sqrt(t))  # sqrt del tempo\n",
    "            i_vals.append(instab)\n",
    "\n",
    "        if d_vals:\n",
    "            m_d.append((np.mean(d_vals), np.std(d_vals)))\n",
    "            m_t.append((np.mean(t_vals), np.std(t_vals)))\n",
    "            m_i.append((np.mean(i_vals), np.std(i_vals)))\n",
    "\n",
    "    def format_metric(pairs):\n",
    "        if not pairs:\n",
    "            return \"nan ± nan\"\n",
    "        mean = np.mean([m for m, _ in pairs])\n",
    "        std = np.mean([s for _, s in pairs])\n",
    "        return f\"{mean:.4f} ± {std:.4f}\"\n",
    "    \n",
    "    if 'NoStability' not in explainer.name:\n",
    "        parameters = f\"({explainer.alpha}, {explainer.beta}, {explainer.gamma})\"\n",
    "    else:\n",
    "        parameters = f\"({explainer.alpha}, {explainer.beta}, X)\"\n",
    "\n",
    "    row = {\n",
    "        \"Parameters\": parameters,\n",
    "        \"M_dissim\": format_metric(m_d),\n",
    "        \"sqrt(M_time)\": format_metric(m_t),\n",
    "        \"M_instab\": format_metric(m_i)\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "if rows:\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    print(f\"Record: {patient_id}_{record_id}\\n\")\n",
    "    print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _______________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last full run: 12/07/25, hour 14:44\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last full run:\", now.strftime(\"%d/%m/%y, hour %H:%M\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
