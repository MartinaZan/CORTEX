{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils_martina.my_utils import *\n",
    "from src.dataset.instances.graph import GraphInstance\n",
    "from src.evaluation.evaluation_metric_ged import GraphEditDistanceMetric\n",
    "from src.evaluation.evaluation_metric_correctness import CorrectnessMetric\n",
    "from src.evaluation.evaluation_metric_fidelity import FidelityMetric\n",
    "from src.utils.metrics.sparsity import SparsityMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25404-Martina\n"
     ]
    }
   ],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "print(file_name)\n",
    "\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get oracle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_record  accuracy  f1_score  recall  precision\n",
      "0            ALL    0.5220    0.6833     1.0     0.5190\n",
      "1       chb01_03    0.5171    0.6811     1.0     0.5165\n",
      "2       chb01_04    0.5268    0.6855     1.0     0.5215\n"
     ]
    }
   ],
   "source": [
    "def get_oracle_metrics(eval_manager):\n",
    "    instances = eval_manager.evaluators[0].dataset.instances\n",
    "    oracle = eval_manager.evaluators[0]._oracle\n",
    "\n",
    "    grouped = {}\n",
    "    for inst in instances:\n",
    "        key = f\"{inst.patient_id}_{inst.record_id}\"\n",
    "        grouped.setdefault(key, []).append(inst)\n",
    "\n",
    "    rows = []\n",
    "    for pr, inst_list in grouped.items():\n",
    "        y_true = [i.label for i in inst_list]\n",
    "        y_pred = [oracle.predict(i) for i in inst_list]\n",
    "        rows.append({\n",
    "            \"patient_record\": pr,\n",
    "            \"accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"f1_score\": round(f1_score(y_true, y_pred),4),\n",
    "            \"recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"precision\": round(precision_score(y_true, y_pred),4)\n",
    "        })\n",
    "\n",
    "    y_true_all = [i.label for i in instances]\n",
    "    y_pred_all = [oracle.predict(i) for i in instances]\n",
    "    global_row = {\n",
    "        \"patient_record\": \"ALL\",\n",
    "        \"accuracy\": round(accuracy_score(y_true_all, y_pred_all),4),\n",
    "        \"f1_score\": round(f1_score(y_true_all, y_pred_all),4),\n",
    "        \"recall\": round(recall_score(y_true_all, y_pred_all),4),\n",
    "        \"precision\": round(precision_score(y_true_all, y_pred_all),4)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([global_row] + rows)\n",
    "\n",
    "oracle_metrics = get_oracle_metrics(eval_manager)\n",
    "print(oracle_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top $k$ counterfactuals and explainer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluator 0: 100%|██████████| 821/821 [09:47<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "ids = {}\n",
    "patient_record = {}\n",
    "correctness = {}\n",
    "fidelity = {}\n",
    "sparsity = {}\n",
    "ged = {}\n",
    "\n",
    "M_dissim = {}\n",
    "M_time = {}\n",
    "M_instab = {}\n",
    "\n",
    "for i in range(len(eval_manager._evaluators)):\n",
    "    # inizializzo liste vuote per l'evaluatore i\n",
    "    ids[i] = []\n",
    "    patient_record[i] = []\n",
    "    correctness[i] = []\n",
    "    fidelity[i] = []\n",
    "    sparsity[i] = []\n",
    "    ged[i] = []\n",
    "\n",
    "    M_dissim[i] = []\n",
    "    M_time[i] = []\n",
    "    M_instab[i] = []\n",
    "\n",
    "    cf_dict = {}\n",
    "\n",
    "    oracle = eval_manager._evaluators[i]._oracle\n",
    "    explainer = eval_manager._evaluators[i]._explainer\n",
    "\n",
    "    list_instances = eval_manager._evaluators[i].dataset.instances\n",
    "    list_instances = [instance for instance in list_instances if instance.label == 1]\n",
    "\n",
    "    for instance in tqdm(list_instances, desc=f\"Evaluator {i}\"):\n",
    "        lista = explainer.explain(instance, return_list=True)\n",
    "        cf_dict[instance.id] = lista\n",
    "\n",
    "        if isinstance(lista, GraphInstance): # Questi sono i casi per cui l'oracolo ha sbagliato\n",
    "            counterfactual = lista\n",
    "        else:\n",
    "            counterfactual = lista[0][1]\n",
    "\n",
    "        ids[i].append(instance.id)\n",
    "        patient_record[i].append(f\"{instance.patient_id}_{instance.record_id}\")\n",
    "        correctness[i].append(CorrectnessMetric().evaluate(instance, counterfactual, oracle, explainer))\n",
    "        fidelity[i].append(FidelityMetric().evaluate(instance, counterfactual, oracle, explainer))\n",
    "        sparsity[i].append(SparsityMetric().evaluate(instance, counterfactual))\n",
    "        ged[i].append(GraphEditDistanceMetric().evaluate(instance, counterfactual))\n",
    "\n",
    "        m_dissim, m_time, m_instab = explainer.compute_metric_components(instance, counterfactual)\n",
    "        M_dissim[i].append(m_dissim)\n",
    "        M_time[i].append(m_time)\n",
    "        M_instab[i].append(m_instab)\n",
    "\n",
    "    with open(f\"output\\cf_dict\\cf_dict_{file_name}_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(cf_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator 0 metrics:\n",
      "  patient_record  correctness  fidelity  sparsity      ged  M_dissim  \\\n",
      "0            ALL          1.0       1.0    1.1015  54.5274    1.5971   \n",
      "1       chb01_03          1.0       1.0    1.0911  53.8775    1.8304   \n",
      "2       chb01_04          1.0       1.0    1.1118  55.1695    1.3667   \n",
      "\n",
      "     M_time  M_instab  \n",
      "0  286.0681    0.7132  \n",
      "1  295.4841    0.7143  \n",
      "2  276.7660    0.7122  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "\n",
    "for i in ids.keys():\n",
    "    dfs[i] = pd.DataFrame({\n",
    "        \"patient_record\": patient_record[i],\n",
    "        \"correctness\": correctness[i],\n",
    "        \"fidelity\": fidelity[i],\n",
    "        \"sparsity\": sparsity[i],\n",
    "        \"ged\": ged[i],\n",
    "        \"M_dissim\": M_dissim[i],\n",
    "        \"M_time\": M_time[i],\n",
    "        \"M_instab\": M_instab[i]\n",
    "    })\n",
    "\n",
    "for i, df in dfs.items():\n",
    "    global_row = df.drop(columns=\"patient_record\").mean().round(4)\n",
    "    global_row[\"patient_record\"] = \"ALL\"\n",
    "    grouped = df.groupby(\"patient_record\").mean().round(4).reset_index()\n",
    "\n",
    "    explainer_metrics = pd.concat([pd.DataFrame([global_row]), grouped], ignore_index=True)\n",
    "    cols = [\"patient_record\", \"correctness\", \"fidelity\", \"sparsity\", \"ged\", \"M_dissim\", \"M_time\", \"M_instab\"]\n",
    "    explainer_metrics = explainer_metrics[cols]\n",
    "\n",
    "    print(f\"Evaluator {i} metrics:\")\n",
    "    print(explainer_metrics)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potrebbe essere utile salvare anche le liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# patient_record = [\n",
    "#     \"ALL\",\n",
    "#     \"chb01_03\",\n",
    "#     \"chb01_04\",\n",
    "#     \"chb01_15\",\n",
    "#     \"chb01_16\",\n",
    "#     \"chb01_18\",\n",
    "#     \"chb01_21\",\n",
    "#     \"chb01_26\"\n",
    "# ]\n",
    "\n",
    "# # Evaluator 0\n",
    "# correctness_0 = [0.9102, 0.9877, 0.9661, 0.9167, 0.8288, 0.7745, 0.9582, 0.9383]\n",
    "# sparsity_0 = [1.0122, 1.1227, 1.0439, 1.0076, 0.9431, 0.8848, 1.0563, 1.0262]\n",
    "# ged_0 = [49.5060, 55.4632, 51.7433, 48.5735, 46.5658, 42.8554, 51.4472, 49.8370]\n",
    "# M_dissim_0 = [1.1638, 1.6573, 1.2546, 1.2869, 1.2631, 0.6223, 0.9581, 1.1037]\n",
    "# M_time_0 = [270.6371, 329.3358, 279.9225, 253.4804, 253.1092, 240.2206, 258.1867, 279.9136]\n",
    "# M_instab_0 = [0.7362, 0.6878, 0.7054, 0.8700, 0.6918, 0.7951, 0.6507, 0.7667]\n",
    "\n",
    "# # Evaluator 1\n",
    "# correctness_1 = [0.9102, 0.9877, 0.9661, 0.9167, 0.8288, 0.7745, 0.9582, 0.9383]\n",
    "# sparsity_1 = [1.0145, 1.1387, 1.0458, 1.0087, 0.9604, 0.8901, 1.0570, 0.9999]\n",
    "# ged_1 = [49.6248, 56.2377, 51.8450, 48.6324, 47.4467, 43.1152, 51.4791, 48.5605]\n",
    "# M_dissim_1 = [1.2083, 1.6976, 1.2964, 1.3393, 1.3504, 0.6366, 1.0067, 1.1310]\n",
    "# M_time_1 = [223.9748, 300.4461, 249.5738, 212.1520, 194.2159, 180.8799, 209.5135, 220.3012]\n",
    "# M_instab_1 = [0.5361, 0.3936, 0.3252, 0.8671, 0.2503, 0.7947, 0.4355, 0.7237]\n",
    "\n",
    "# # Evaluator 2\n",
    "# correctness_2 = [0.9102, 0.9877, 0.9661, 0.9167, 0.8288, 0.7745, 0.9582, 0.9383]\n",
    "# sparsity_2 = [1.0140, 1.1222, 1.0414, 1.0063, 0.9561, 0.8948, 1.0631, 1.0130]\n",
    "# ged_2 = [49.5943, 55.4436, 51.6126, 48.5172, 47.2208, 43.3431, 51.7715, 49.2000]\n",
    "# M_dissim_2 = [1.1880, 1.6820, 1.2743, 1.3178, 1.3073, 0.6299, 0.9833, 1.1207]\n",
    "# M_time_2 = [230.9660, 296.0098, 249.6392, 221.0980, 202.2060, 196.6152, 219.8993, 230.6840]\n",
    "# M_instab_2 = [0.6509, 0.5598, 0.5404, 0.8787, 0.4866, 0.8145, 0.5473, 0.7549]\n",
    "\n",
    "# # Costruzione dei dataframe\n",
    "# dfs = {\n",
    "#     0: pd.DataFrame({\n",
    "#         \"patient_record\": patient_record,\n",
    "#         \"correctness\": correctness_0,\n",
    "#         \"fidelity\": correctness_0,\n",
    "#         \"sparsity\": sparsity_0,\n",
    "#         \"ged\": ged_0,\n",
    "#         \"M_dissim\": M_dissim_0,\n",
    "#         \"M_time\": M_time_0,\n",
    "#         \"M_instab\": M_instab_0,\n",
    "#     }),\n",
    "#     1: pd.DataFrame({\n",
    "#         \"patient_record\": patient_record,\n",
    "#         \"correctness\": correctness_1,\n",
    "#         \"fidelity\": correctness_1,\n",
    "#         \"sparsity\": sparsity_1,\n",
    "#         \"ged\": ged_1,\n",
    "#         \"M_dissim\": M_dissim_1,\n",
    "#         \"M_time\": M_time_1,\n",
    "#         \"M_instab\": M_instab_1,\n",
    "#     }),\n",
    "#     2: pd.DataFrame({\n",
    "#         \"patient_record\": patient_record,\n",
    "#         \"correctness\": correctness_2,\n",
    "#         \"fidelity\": correctness_2,\n",
    "#         \"sparsity\": sparsity_2,\n",
    "#         \"ged\": ged_2,\n",
    "#         \"M_dissim\": M_dissim_2,\n",
    "#         \"M_time\": M_time_2,\n",
    "#         \"M_instab\": M_instab_2,\n",
    "#     }),\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
