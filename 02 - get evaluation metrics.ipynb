{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils_martina.my_utils import *\n",
    "from src.dataset.instances.graph import GraphInstance\n",
    "from src.evaluation.evaluation_metric_ged import GraphEditDistanceMetric\n",
    "from src.evaluation.evaluation_metric_correctness import CorrectnessMetric\n",
    "from src.evaluation.evaluation_metric_fidelity import FidelityMetric\n",
    "from src.utils.metrics.sparsity import SparsityMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3788-Martina\n"
     ]
    }
   ],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "print(file_name)\n",
    "\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get oracle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_record  accuracy  f1_score  recall  precision\n",
      "0            ALL    0.9040    0.9088  0.9102     0.9074\n",
      "1       chb01_03    0.9279    0.9340  0.9877     0.8857\n",
      "2       chb01_04    0.9576    0.9591  0.9661     0.9523\n",
      "3       chb01_15    0.7901    0.8184  0.9167     0.7391\n",
      "4       chb01_16    0.9041    0.8991  0.8288     0.9824\n",
      "5       chb01_18    0.8714    0.8646  0.7745     0.9783\n",
      "6       chb01_21    0.9380    0.9455  0.9582     0.9330\n",
      "7       chb01_26    0.9411    0.9441  0.9383     0.9500\n"
     ]
    }
   ],
   "source": [
    "def get_oracle_metrics(eval_manager):\n",
    "    instances = eval_manager.evaluators[0].dataset.instances\n",
    "    oracle = eval_manager.evaluators[0]._oracle\n",
    "\n",
    "    grouped = {}\n",
    "    for inst in instances:\n",
    "        key = f\"{inst.patient_id}_{inst.record_id}\"\n",
    "        grouped.setdefault(key, []).append(inst)\n",
    "\n",
    "    rows = []\n",
    "    for pr, inst_list in grouped.items():\n",
    "        y_true = [i.label for i in inst_list]\n",
    "        y_pred = [oracle.predict(i) for i in inst_list]\n",
    "        rows.append({\n",
    "            \"patient_record\": pr,\n",
    "            \"accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"f1_score\": round(f1_score(y_true, y_pred),4),\n",
    "            \"recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"precision\": round(precision_score(y_true, y_pred),4)\n",
    "        })\n",
    "\n",
    "    y_true_all = [i.label for i in instances]\n",
    "    y_pred_all = [oracle.predict(i) for i in instances]\n",
    "    global_row = {\n",
    "        \"patient_record\": \"ALL\",\n",
    "        \"accuracy\": round(accuracy_score(y_true_all, y_pred_all),4),\n",
    "        \"f1_score\": round(f1_score(y_true_all, y_pred_all),4),\n",
    "        \"recall\": round(recall_score(y_true_all, y_pred_all),4),\n",
    "        \"precision\": round(precision_score(y_true_all, y_pred_all),4)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([global_row] + rows)\n",
    "\n",
    "oracle_metrics = get_oracle_metrics(eval_manager)\n",
    "print(oracle_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top $k$ counterfactuals and explainer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluator 0: 100%|██████████| 2596/2596 [1:50:25<00:00,  2.55s/it]  \n"
     ]
    }
   ],
   "source": [
    "ids = {}\n",
    "patient_record = {}\n",
    "correctness = {}\n",
    "fidelity = {}\n",
    "sparsity = {}\n",
    "ged = {}\n",
    "\n",
    "M_dissim = {}\n",
    "M_time = {}\n",
    "M_instab = {}\n",
    "\n",
    "for i in range(len(eval_manager._evaluators)):\n",
    "    ids[i] = []\n",
    "    patient_record[i] = []\n",
    "    correctness[i] = []\n",
    "    fidelity[i] = []\n",
    "    sparsity[i] = []\n",
    "    ged[i] = []\n",
    "\n",
    "    M_dissim[i] = []\n",
    "    M_time[i] = []\n",
    "    M_instab[i] = []\n",
    "\n",
    "    cf_dict = {}\n",
    "\n",
    "    oracle = eval_manager._evaluators[i]._oracle\n",
    "    explainer = eval_manager._evaluators[i]._explainer\n",
    "\n",
    "    list_instances = eval_manager._evaluators[i].dataset.instances\n",
    "    list_instances = [instance for instance in list_instances if (instance.label == 1 and oracle.predict(instance) == 1)]\n",
    "\n",
    "    num_params = len(inspect.signature(explainer.explain).parameters)\n",
    "\n",
    "    for instance in tqdm(list_instances, desc=f\"Evaluator {i}\"):\n",
    "        # Estraggo metriche solo per istanze corrette\n",
    "        if num_params == 2:\n",
    "            result = explainer.explain(instance, return_list=True)\n",
    "        else:\n",
    "            result = explainer.explain(instance)\n",
    "        \n",
    "        cf_dict[instance.id] = result\n",
    "\n",
    "        if isinstance(result, GraphInstance):\n",
    "            counterfactual = result\n",
    "        else:\n",
    "            counterfactual = result[0][1]\n",
    "\n",
    "        ids[i].append(instance.id)\n",
    "        patient_record[i].append(f\"{instance.patient_id}_{instance.record_id}\")\n",
    "        correctness[i].append(CorrectnessMetric().evaluate(instance, counterfactual, oracle, explainer))\n",
    "        fidelity[i].append(FidelityMetric().evaluate(instance, counterfactual, oracle, explainer))\n",
    "        sparsity[i].append(SparsityMetric().evaluate(instance, counterfactual))\n",
    "        ged[i].append(GraphEditDistanceMetric().evaluate(instance, counterfactual))\n",
    "\n",
    "        if num_params == 2:\n",
    "            m_dissim, m_time, m_instab = explainer.compute_metric_components(instance, counterfactual)\n",
    "            M_dissim[i].append(m_dissim)\n",
    "            M_time[i].append(m_time)\n",
    "            M_instab[i].append(m_instab)\n",
    "        else:\n",
    "            M_dissim[i].append(np.nan)\n",
    "            M_time[i].append(np.nan)\n",
    "            M_instab[i].append(np.nan)\n",
    "\n",
    "    with open(f\"output\\cf_dict\\cf_dict_{file_name}_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(cf_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary over different explainers, only top counterfactual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for i in ids.keys():\n",
    "    dfs[i] = pd.DataFrame({\n",
    "        \"patient_record\": patient_record[i],\n",
    "        \"correctness\": correctness[i],\n",
    "        \"fidelity\": fidelity[i],\n",
    "        \"sparsity\": sparsity[i],\n",
    "        \"ged\": ged[i],\n",
    "        \"M_dissim\": M_dissim[i],\n",
    "        \"M_time\": np.sqrt(M_time[i]),\n",
    "        \"M_instab\": M_instab[i]\n",
    "    })\n",
    "\n",
    "with open(f\"output\\dfs\\dfs_{file_name.split('-')[0]}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator 0 metrics:\n",
      "  patient_record  correctness  fidelity  M_dissim\n",
      "0            ALL          1.0       1.0    1.2762\n",
      "1       chb01_03          1.0       1.0    1.6761\n",
      "2       chb01_04          1.0       1.0    1.2977\n",
      "3       chb01_15          1.0       1.0    1.3996\n",
      "4       chb01_16          1.0       1.0    1.5184\n",
      "5       chb01_18          1.0       1.0    0.8033\n",
      "6       chb01_21          1.0       1.0    0.9974\n",
      "7       chb01_26          1.0       1.0    1.1744\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(f\"output\\dfs\\dfs_{file_name.split('-')[0]}.pkl\", \"rb\") as f:\n",
    "    dfs = pickle.load(f)\n",
    "\n",
    "for i, df in dfs.items():\n",
    "    global_row = df.drop(columns=\"patient_record\").mean().round(4)\n",
    "    global_row[\"patient_record\"] = \"ALL\"\n",
    "    grouped = df.groupby(\"patient_record\").mean().round(4).reset_index()\n",
    "\n",
    "    explainer_metrics = pd.concat([pd.DataFrame([global_row]), grouped], ignore_index=True)\n",
    "    cols = [\"patient_record\", \"correctness\", \"fidelity\", \"M_dissim\"]\n",
    "    explainer_metrics = explainer_metrics[cols]\n",
    "\n",
    "    print(f\"Evaluator {i} metrics:\")\n",
    "    print(explainer_metrics)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# ADD PLAUSIBILITY!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator 0 metrics:\n",
      "  patient_record      correctness         fidelity         M_dissim  \\\n",
      "0            ALL  1.0000 ± 0.0000  1.0000 ± 0.0000  1.2762 ± 0.4641   \n",
      "1       chb01_03  1.0000 ± 0.0000  1.0000 ± 0.0000  1.6761 ± 0.5261   \n",
      "2       chb01_04  1.0000 ± 0.0000  1.0000 ± 0.0000  1.2977 ± 0.4933   \n",
      "3       chb01_15  1.0000 ± 0.0000  1.0000 ± 0.0000  1.3996 ± 0.3442   \n",
      "4       chb01_16  1.0000 ± 0.0000  1.0000 ± 0.0000  1.5184 ± 0.3811   \n",
      "5       chb01_18  1.0000 ± 0.0000  1.0000 ± 0.0000  0.8033 ± 0.2419   \n",
      "6       chb01_21  1.0000 ± 0.0000  1.0000 ± 0.0000  0.9974 ± 0.2123   \n",
      "7       chb01_26  1.0000 ± 0.0000  1.0000 ± 0.0000  1.1744 ± 0.2693   \n",
      "\n",
      "          sqrt(M_time)         M_instab  \n",
      "0  280.8730 ± 178.1943  0.7316 ± 0.2607  \n",
      "1  356.1451 ± 154.9936  0.6732 ± 0.2884  \n",
      "2  241.4067 ± 147.8887  0.7085 ± 0.3068  \n",
      "3  226.6177 ± 153.1784  0.8579 ± 0.1551  \n",
      "4  341.7298 ± 192.6924  0.7107 ± 0.2755  \n",
      "5  371.4191 ± 184.7661  0.7797 ± 0.2535  \n",
      "6   141.4171 ± 80.2202  0.6599 ± 0.2400  \n",
      "7  310.2230 ± 191.6383  0.7457 ± 0.2188  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for i, df in dfs.items():\n",
    "    def format_mean_std(row):\n",
    "        return f\"{row['mean']:.4f} ± {row['std']:.4f}\"\n",
    "    \n",
    "    # Calcolo globale\n",
    "    global_stats = df.drop(columns=\"patient_record\").agg(['mean', 'std']).T\n",
    "    global_stats['formatted'] = global_stats.apply(format_mean_std, axis=1)\n",
    "    global_row = global_stats['formatted'].to_frame().T\n",
    "    global_row['patient_record'] = 'ALL'\n",
    "    \n",
    "    # Calcolo per paziente\n",
    "    grouped_stats = df.groupby('patient_record').agg(['mean', 'std'])\n",
    "    \n",
    "    formatted_cols = pd.DataFrame()\n",
    "    for col in grouped_stats.columns.levels[0]:\n",
    "        formatted_cols[col] = grouped_stats[col].apply(format_mean_std, axis=1)\n",
    "    \n",
    "    formatted_cols['patient_record'] = grouped_stats.index\n",
    "    \n",
    "    # Unisco globale e raggruppato\n",
    "    explainer_metrics = pd.concat([global_row, formatted_cols], ignore_index=True)\n",
    "    explainer_metrics = explainer_metrics[[\"patient_record\", \"correctness\", \"fidelity\", \"M_dissim\", \"M_time\", \"M_instab\"]]\n",
    "    explainer_metrics = explainer_metrics.rename(columns={\"M_time\": \"sqrt(M_time)\"})\n",
    "    \n",
    "    print(f\"Evaluator {i} metrics:\")\n",
    "    print(explainer_metrics)\n",
    "    print(\"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation study (single patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = \"chb01\"\n",
    "record_id = \"03\"\n",
    "\n",
    "M_dissim_mean = []\n",
    "M_time_mean = []\n",
    "M_instab_mean = []\n",
    "\n",
    "M_dissim_std = []\n",
    "M_time_std = []\n",
    "M_instab_std = []\n",
    "\n",
    "for instance in list_instances:\n",
    "    if instance.patient_id == patient_id and instance.record_id == record_id:\n",
    "        M_dissim_counterfactuals = []\n",
    "        M_time_counterfactuals = []\n",
    "        M_instab_counterfactuals = []\n",
    "\n",
    "        for counterfactual in cf_dict[instance.id]:\n",
    "            m_dissim, m_time, m_instab = explainer.compute_metric_components(instance, counterfactual)\n",
    "            M_dissim_counterfactuals.append(m_dissim)\n",
    "            M_time_counterfactuals.append(m_time)\n",
    "            M_instab_counterfactuals.append(m_instab)\n",
    "        \n",
    "        M_dissim_mean.append(np.mean(M_dissim_counterfactuals))\n",
    "        M_dissim_std.append(np.std(M_dissim_counterfactuals))\n",
    "        M_time_mean.append(np.mean(M_time_counterfactuals))\n",
    "        M_time_std.append(np.std(M_time_counterfactuals))\n",
    "        M_instab_mean.append(np.mean(M_instab_counterfactuals))\n",
    "        M_instab_std.append(np.std(M_instab_counterfactuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chb01_03\n",
      "\n",
      "Evaluator 0:\n",
      "M_dissim:       1.7066 ± 0.0266\n",
      "sqrt(M_time): 360.8770 ± 123.9334\n",
      "M_instab:       0.7076 ± 0.2307\n"
     ]
    }
   ],
   "source": [
    "patient_id = \"chb01\"\n",
    "record_id = \"03\"\n",
    "\n",
    "print(f\"{patient_id}_{record_id}\")\n",
    "\n",
    "for i in range(len(eval_manager._evaluators)):\n",
    "    explainer = eval_manager._evaluators[i]._explainer\n",
    "\n",
    "    M_dissim_mean = []\n",
    "    M_time_mean = []\n",
    "    M_instab_mean = []\n",
    "\n",
    "    M_dissim_std = []\n",
    "    M_time_std = []\n",
    "    M_instab_std = []\n",
    "\n",
    "    for instance in list_instances:\n",
    "        if instance.patient_id == patient_id and instance.record_id == record_id:\n",
    "            M_dissim_counterfactuals = []\n",
    "            M_time_counterfactuals = []\n",
    "            M_instab_counterfactuals = []\n",
    "\n",
    "            for counterfactual in cf_dict[instance.id]:\n",
    "                m_dissim, m_time, m_instab = explainer.compute_metric_components(instance, counterfactual[1])\n",
    "                M_dissim_counterfactuals.append(m_dissim)\n",
    "                M_time_counterfactuals.append(np.sqrt(m_time))\n",
    "                M_instab_counterfactuals.append(m_instab)\n",
    "\n",
    "            M_dissim_mean.append(np.mean(M_dissim_counterfactuals))\n",
    "            M_dissim_std.append(np.std(M_dissim_counterfactuals))\n",
    "            M_time_mean.append(np.mean(M_time_counterfactuals))\n",
    "            M_time_std.append(np.std(M_time_counterfactuals))\n",
    "            M_instab_mean.append(np.mean(M_instab_counterfactuals))\n",
    "            M_instab_std.append(np.std(M_instab_counterfactuals))\n",
    "\n",
    "    # Stampa aggregata per questo evaluator\n",
    "    print(f\"\\nEvaluator {i}:\")\n",
    "    print(f\"M_dissim:       {np.mean(M_dissim_mean):.4f} ± {np.mean(M_dissim_std):.4f}\")\n",
    "    print(f\"sqrt(M_time): {np.mean(M_time_mean):.4f} ± {np.mean(M_time_std):.4f}\")\n",
    "    print(f\"M_instab:       {np.mean(M_instab_mean):.4f} ± {np.mean(M_instab_std):.4f}\")\n",
    "    print(\"\")\n",
    "\n",
    "# DA CONTROLLARE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _______________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
