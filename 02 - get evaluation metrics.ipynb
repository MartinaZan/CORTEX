{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils_martina.my_utils import *\n",
    "from src.dataset.instances.graph import GraphInstance\n",
    "from src.evaluation.evaluation_metric_correctness import CorrectnessMetric\n",
    "from src.evaluation.evaluation_metric_fidelity import FidelityMetric\n",
    "from src.evaluation.evaluation_metric_implausibility import ImplausibilityMetric\n",
    "from src.evaluation.evaluation_metric_dissimilarity import M_dissim_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3788-Martina\n"
     ]
    }
   ],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "print(file_name)\n",
    "\n",
    "# file_name = \"23256-Martina\"\n",
    "\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oracle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_record  accuracy  f1_score  recall  precision\n",
      "0            ALL    0.9040    0.9088  0.9102     0.9074\n",
      "1       chb01_03    0.9279    0.9340  0.9877     0.8857\n",
      "2       chb01_04    0.9576    0.9591  0.9661     0.9523\n",
      "3       chb01_15    0.7901    0.8184  0.9167     0.7391\n",
      "4       chb01_16    0.9041    0.8991  0.8288     0.9824\n",
      "5       chb01_18    0.8714    0.8646  0.7745     0.9783\n",
      "6       chb01_21    0.9380    0.9455  0.9582     0.9330\n",
      "7       chb01_26    0.9411    0.9441  0.9383     0.9500\n"
     ]
    }
   ],
   "source": [
    "def get_oracle_metrics(eval_manager):\n",
    "    instances = eval_manager.evaluators[0].dataset.instances\n",
    "    oracle = eval_manager.evaluators[0]._oracle\n",
    "\n",
    "    grouped = {}\n",
    "    for inst in instances:\n",
    "        key = f\"{inst.patient_id}_{inst.record_id}\"\n",
    "        grouped.setdefault(key, []).append(inst)\n",
    "\n",
    "    rows = []\n",
    "    for pr, inst_list in grouped.items():\n",
    "        y_true = [i.label for i in inst_list]\n",
    "        y_pred = [oracle.predict(i) for i in inst_list]\n",
    "        rows.append({\n",
    "            \"patient_record\": pr,\n",
    "            \"accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"f1_score\": round(f1_score(y_true, y_pred),4),\n",
    "            \"recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"precision\": round(precision_score(y_true, y_pred),4)\n",
    "        })\n",
    "\n",
    "    y_true_all = [i.label for i in instances]\n",
    "    y_pred_all = [oracle.predict(i) for i in instances]\n",
    "    global_row = {\n",
    "        \"patient_record\": \"ALL\",\n",
    "        \"accuracy\": round(accuracy_score(y_true_all, y_pred_all),4),\n",
    "        \"f1_score\": round(f1_score(y_true_all, y_pred_all),4),\n",
    "        \"recall\": round(recall_score(y_true_all, y_pred_all),4),\n",
    "        \"precision\": round(precision_score(y_true_all, y_pred_all),4)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([global_row] + rows)\n",
    "\n",
    "oracle_metrics = get_oracle_metrics(eval_manager)\n",
    "print(oracle_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top $k$ counterfactuals and explainer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluator 0: 100%|██████████| 10/10 [00:44<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "metrics_keys = ['ID', 'Correctness', 'Fidelity', 'Implausibility', 'Dissimilarity']\n",
    "metrics = {k: {} for k in metrics_keys}\n",
    "\n",
    "correctness_metric = CorrectnessMetric()\n",
    "fidelity_metric = FidelityMetric()\n",
    "implausibility_metric = ImplausibilityMetric()\n",
    "dissimilarity_metric = M_dissim_metric()\n",
    "\n",
    "for i in range(len(eval_manager._evaluators)):\n",
    "    # Initialize empty lists for each metric for this evaluator\n",
    "    for key in metrics_keys:\n",
    "        metrics[key][i] = []\n",
    "\n",
    "    cf_dict = {}\n",
    "\n",
    "    evaluator = eval_manager._evaluators[i]\n",
    "    oracle = evaluator._oracle\n",
    "    explainer = evaluator._explainer\n",
    "    dataset = evaluator.dataset.instances\n",
    "\n",
    "    # Select only correctly predicted positive instances\n",
    "    list_instances = [\n",
    "        inst for inst in dataset\n",
    "        if inst.label == 1 and oracle.predict(inst) == 1\n",
    "    ]\n",
    "\n",
    "    # Check if the explainer's explain method takes two arguments (i.e., has return_list=True)\n",
    "    has_return_list = len(inspect.signature(explainer.explain).parameters) == 2\n",
    "\n",
    "    for instance in tqdm(list_instances, desc=f\"Evaluator {i}\"):\n",
    "        # Generate counterfactual for the instance\n",
    "        result = explainer.explain(instance, return_list=True) if has_return_list else explainer.explain(instance)\n",
    "        cf_dict[instance.id] = result\n",
    "\n",
    "        # Extract the counterfactual instance\n",
    "        counterfactual = result if isinstance(result, GraphInstance) else result[0][1]\n",
    "\n",
    "        # Record info and metrics\n",
    "        metrics['ID'][i].append(instance.id)\n",
    "        metrics['Correctness'][i].append(correctness_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Fidelity'][i].append(fidelity_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Implausibility'][i].append(implausibility_metric.evaluate(instance, counterfactual, dataset=dataset))\n",
    "        metrics['Dissimilarity'][i].append(dissimilarity_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "\n",
    "    with open(f\"output/cf_dict/cf_dict_{file_name}_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(cf_dict, f)\n",
    "\n",
    "    with open(f\"output/cf_dict/metrics_{file_name}_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainer metrics (all patients, only top counterfactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Correctness  Fidelity  Implausibility  Dissimilarity\n",
      "Evaluator                                                      \n",
      "0                  1.0       1.0             0.0         0.7918\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for eval_id in metrics['ID'].keys():\n",
    "    row = {\"Evaluator\": eval_id}\n",
    "    for metric in metrics_keys[1:]:\n",
    "        row[metric] = np.mean(metrics[metric][eval_id]).round(4)\n",
    "    rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_df.set_index(\"Evaluator\", inplace=True)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation study (single patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record: chb01_03\n",
      "\n",
      "                  M_dissim         sqrt(M_time)         M_instab\n",
      "Evaluator                                                       \n",
      "0          0.8071 ± 0.0203  316.0506 ± 124.5942  0.6784 ± 0.2537\n"
     ]
    }
   ],
   "source": [
    "patient_id = \"chb01\"\n",
    "record_id = \"03\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, evaluator in enumerate(eval_manager._evaluators):\n",
    "    explainer = evaluator._explainer\n",
    "\n",
    "    if not 'Temporal' in explainer.name:\n",
    "        continue\n",
    "\n",
    "    m_d, m_t, m_i = [], [], []\n",
    "\n",
    "    for instance in list_instances:\n",
    "        if instance.patient_id != patient_id or instance.record_id != record_id:\n",
    "            continue\n",
    "\n",
    "        d_vals, t_vals, i_vals = [], [], []\n",
    "\n",
    "        for cf in cf_dict[instance.id]:\n",
    "            d, t, instab = explainer.compute_metric_components(instance, cf[1])\n",
    "            d_vals.append(d)\n",
    "            t_vals.append(np.sqrt(t))  # Apply sqrt here\n",
    "            i_vals.append(instab)\n",
    "\n",
    "        if d_vals:\n",
    "            m_d.append((np.mean(d_vals), np.std(d_vals)))\n",
    "            m_t.append((np.mean(t_vals), np.std(t_vals)))\n",
    "            m_i.append((np.mean(i_vals), np.std(i_vals)))\n",
    "\n",
    "    # Aggregate across instances (per evaluator)\n",
    "    def format_metric(pairs):\n",
    "        if not pairs:\n",
    "            return \"nan ± nan\"\n",
    "        mean = np.mean([m for m, _ in pairs])\n",
    "        std = np.mean([s for _, s in pairs])\n",
    "        return f\"{mean:.4f} ± {std:.4f}\"\n",
    "\n",
    "    row = {\n",
    "        \"Evaluator\": i,\n",
    "        \"M_dissim\": format_metric(m_d),\n",
    "        \"sqrt(M_time)\": format_metric(m_t),\n",
    "        \"M_instab\": format_metric(m_i)\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "if rows != []:\n",
    "    summary_df = pd.DataFrame(rows).set_index(\"Evaluator\")\n",
    "    print(f\"Record: {patient_id}_{record_id}\\n\")\n",
    "    print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _______________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last full run: 03/07/25, hour 10:59\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(\"Last full run:\", now.strftime(\"%d/%m/%y, hour %H:%M\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
