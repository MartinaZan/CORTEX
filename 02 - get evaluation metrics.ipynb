{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils_martina.my_utils import *\n",
    "from src.dataset.instances.graph import GraphInstance\n",
    "from src.evaluation.evaluation_metric_ged import GraphEditDistanceMetric\n",
    "from src.evaluation.evaluation_metric_correctness import CorrectnessMetric\n",
    "from src.evaluation.evaluation_metric_fidelity import FidelityMetric\n",
    "from src.utils.metrics.sparsity import SparsityMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22268-Martina\n"
     ]
    }
   ],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "print(file_name)\n",
    "\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get oracle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_record  accuracy  f1_score  recall  precision\n",
      "0            ALL    0.9454    0.9459  0.9257     0.9669\n",
      "1       chb01_03    0.9583    0.9595  0.9583     0.9607\n",
      "2       chb01_04    0.9326    0.9318  0.8935     0.9736\n"
     ]
    }
   ],
   "source": [
    "def get_oracle_metrics(eval_manager):\n",
    "    instances = eval_manager.evaluators[0].dataset.instances\n",
    "    oracle = eval_manager.evaluators[0]._oracle\n",
    "\n",
    "    grouped = {}\n",
    "    for inst in instances:\n",
    "        key = f\"{inst.patient_id}_{inst.record_id}\"\n",
    "        grouped.setdefault(key, []).append(inst)\n",
    "\n",
    "    rows = []\n",
    "    for pr, inst_list in grouped.items():\n",
    "        y_true = [i.label for i in inst_list]\n",
    "        y_pred = [oracle.predict(i) for i in inst_list]\n",
    "        rows.append({\n",
    "            \"patient_record\": pr,\n",
    "            \"accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"f1_score\": round(f1_score(y_true, y_pred),4),\n",
    "            \"recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"precision\": round(precision_score(y_true, y_pred),4)\n",
    "        })\n",
    "\n",
    "    y_true_all = [i.label for i in instances]\n",
    "    y_pred_all = [oracle.predict(i) for i in instances]\n",
    "    global_row = {\n",
    "        \"patient_record\": \"ALL\",\n",
    "        \"accuracy\": round(accuracy_score(y_true_all, y_pred_all),4),\n",
    "        \"f1_score\": round(f1_score(y_true_all, y_pred_all),4),\n",
    "        \"recall\": round(recall_score(y_true_all, y_pred_all),4),\n",
    "        \"precision\": round(precision_score(y_true_all, y_pred_all),4)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([global_row] + rows)\n",
    "\n",
    "oracle_metrics = get_oracle_metrics(eval_manager)\n",
    "print(oracle_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top $k$ counterfactuals and explainer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluator 0: 100%|██████████| 821/821 [48:20<00:00,  3.53s/it]\n"
     ]
    }
   ],
   "source": [
    "ids = {}\n",
    "patient_record = {}\n",
    "correctness = {}\n",
    "fidelity = {}\n",
    "sparsity = {}\n",
    "ged = {}\n",
    "\n",
    "M_dissim = {}\n",
    "M_time = {}\n",
    "M_instab = {}\n",
    "\n",
    "for i in range(len(eval_manager._evaluators)):\n",
    "    # inizializzo liste vuote per l'evaluatore i\n",
    "    ids[i] = []\n",
    "    patient_record[i] = []\n",
    "    correctness[i] = []\n",
    "    fidelity[i] = []\n",
    "    sparsity[i] = []\n",
    "    ged[i] = []\n",
    "\n",
    "    M_dissim[i] = []\n",
    "    M_time[i] = []\n",
    "    M_instab[i] = []\n",
    "\n",
    "    cf_dict = {}\n",
    "\n",
    "    oracle = eval_manager._evaluators[i]._oracle\n",
    "    explainer = eval_manager._evaluators[i]._explainer\n",
    "\n",
    "    list_instances = eval_manager._evaluators[i].dataset.instances\n",
    "    list_instances = [instance for instance in list_instances if instance.label == 1]\n",
    "\n",
    "    for instance in tqdm(list_instances, desc=f\"Evaluator {i}\"):\n",
    "        lista = explainer.explain(instance, return_list=True)\n",
    "        cf_dict[instance.id] = lista\n",
    "\n",
    "        if isinstance(lista, GraphInstance): # Questi sono i casi per cui l'oracolo ha sbagliato\n",
    "            counterfactual = lista\n",
    "        else:\n",
    "            counterfactual = lista[0][1]\n",
    "\n",
    "        ids[i].append(instance.id)\n",
    "        patient_record[i].append(f\"{instance.patient_id}_{instance.record_id}\")\n",
    "        correctness[i].append(CorrectnessMetric().evaluate(instance, counterfactual, oracle, explainer))\n",
    "        fidelity[i].append(FidelityMetric().evaluate(instance, counterfactual, oracle, explainer))\n",
    "        sparsity[i].append(SparsityMetric().evaluate(instance, counterfactual))\n",
    "        ged[i].append(GraphEditDistanceMetric().evaluate(instance, counterfactual))\n",
    "\n",
    "        m_dissim, m_time, m_instab = explainer.compute_metric_components(instance, counterfactual)\n",
    "        M_dissim[i].append(m_dissim)\n",
    "        M_time[i].append(m_time)\n",
    "        M_instab[i].append(m_instab)\n",
    "\n",
    "    with open(f\"output\\cf_dict\\cf_dict_{file_name}_{i}.pkl\", 'wb') as f:\n",
    "        pickle.dump(cf_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator 0 metrics:\n",
      "  patient_record  correctness  fidelity  sparsity      ged\n",
      "0            ALL       0.9257    0.9257    1.0216  50.5749\n",
      "1       chb01_03       0.9583    0.9583    1.0844  53.6152\n",
      "2       chb01_04       0.8935    0.8935    0.9596  47.5714\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "\n",
    "for i in ids.keys():\n",
    "    dfs[i] = pd.DataFrame({\n",
    "        \"patient_record\": patient_record[i],\n",
    "        \"correctness\": correctness[i],\n",
    "        \"fidelity\": fidelity[i],\n",
    "        \"sparsity\": sparsity[i],\n",
    "        \"ged\": ged[i],\n",
    "        \"M_dissim\": M_dissim[i],\n",
    "        \"M_time\": M_time[i],\n",
    "        \"M_instab\": M_instab[i]\n",
    "    })\n",
    "\n",
    "for i, df in dfs.items():\n",
    "    global_row = df.drop(columns=\"patient_record\").mean().round(4)\n",
    "    global_row[\"patient_record\"] = \"ALL\"\n",
    "    grouped = df.groupby(\"patient_record\").mean().round(4).reset_index()\n",
    "\n",
    "    explainer_metrics = pd.concat([pd.DataFrame([global_row]), grouped], ignore_index=True)\n",
    "    cols = [\"patient_record\", \"correctness\", \"fidelity\", \"sparsity\", \"ged\"]\n",
    "    explainer_metrics = explainer_metrics[cols]\n",
    "\n",
    "    print(f\"Evaluator {i} metrics:\")\n",
    "    print(explainer_metrics)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
