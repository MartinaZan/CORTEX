{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0beb8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils_martina.my_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "008097f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = ['19316', '19708', '1796', '22248', '12236', '20296']\n",
    "conta = [2852, 2870, 2246, 1622, 1622, 1238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b206c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_keys = ['ID', 'Correctness', 'Fidelity', 'Implausibility', 'Dissimilarity', 'Accuracy']\n",
    "dfs = []\n",
    "\n",
    "for i in range(len(logs)):\n",
    "    rows = []\n",
    "\n",
    "    with open(f\"output/cf_dict/metrics_{logs[i]}-Martina.pkl\", 'rb') as f:\n",
    "        metrics = pickle.load(f)\n",
    "\n",
    "    for eval_id in metrics['ID'].keys():\n",
    "        row = {\"Evaluator\": eval_id}\n",
    "        acc = np.array(metrics['Accuracy'][eval_id])\n",
    "\n",
    "        for metric in metrics_keys[1:]:\n",
    "            vals = np.array(metrics[metric][eval_id])\n",
    "\n",
    "            # Implausibility and Dissimilarity are only computed for well-predicted instances\n",
    "            if metric in ['Implausibility', 'Dissimilarity']:\n",
    "                filtered_vals = vals[acc == True]\n",
    "                mean_val = np.mean(filtered_vals) if filtered_vals.size > 0 else np.nan\n",
    "            else:\n",
    "                mean_val = np.mean(vals) if vals.size > 0 else np.nan\n",
    "            row[metric] = round(mean_val, 4) if not np.isnan(mean_val) else np.nan\n",
    "            \n",
    "        rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(rows).set_index(\"Evaluator\")\n",
    "    dfs.append(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "833ace4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Correctness  Fidelity  Implausibility  Dissimilarity  Accuracy\n",
      "Evaluator                                                                \n",
      "0               0.2254    0.1852          0.2749         0.2780    0.8906\n",
      "1               0.3714    0.2722          0.2112         0.2150    0.8906\n",
      "2               0.8908    0.8904          0.0000         0.8951    0.8906\n",
      "3               0.9966    0.7812          0.0000         0.8468    0.8906\n",
      "4               0.3080    0.1714             NaN            NaN    0.8906\n"
     ]
    }
   ],
   "source": [
    "# Verifica che tutti abbiano la stessa forma\n",
    "shape = dfs[0].shape\n",
    "assert all(df.shape == shape for df in dfs), \"DataFrame shapes must match\"\n",
    "\n",
    "# Stack in un array 3D\n",
    "array_3d = np.array([df.values for df in dfs])  # shape: (n_df, rows, cols)\n",
    "weights = np.array(conta).reshape(-1, 1, 1)   # shape: (n_df, 1, 1)\n",
    "\n",
    "# Verifica se ci sono NaN in una qualsiasi posizione tra i DataFrame\n",
    "nan_mask = np.any(np.isnan(array_3d), axis=0)  # shape: (rows, cols)\n",
    "\n",
    "# Calcolo media pesata\n",
    "weighted_sum = np.sum(array_3d * weights, axis=0)\n",
    "total_weight = np.sum(weights)\n",
    "\n",
    "weighted_avg = weighted_sum / total_weight\n",
    "\n",
    "# Dove c'era almeno un NaN â†’ mettiamo NaN anche nella media\n",
    "weighted_avg[nan_mask] = np.nan\n",
    "\n",
    "# Ricostruzione DataFrame\n",
    "result_df = pd.DataFrame(np.round(weighted_avg,4), columns=dfs[0].columns, index=dfs[0].index)\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ba6c843c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0) DataDrivenBidirectionalSearchExplainer\n",
      "(1) ObliviousBidirectionalSearchExplainer\n",
      "(2) TemporalDCESExplainer: (0.7, 0.2, 0.1)\n",
      "(3) TemporalDCESExplainerNoStability: (1, 0, X)\n",
      "(4) GNNMOExp\n"
     ]
    }
   ],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "with open(f'{eval_manager_path}{logs[0]}-Martina.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)\n",
    "\n",
    "for i, evaluator in enumerate(eval_manager._evaluators):\n",
    "    explainer = evaluator._explainer\n",
    "    name = explainer.name.split('-')[0]\n",
    "    if \"Temporal\" in name:\n",
    "        if 'NoStability' not in name:\n",
    "            print(f\"({i}) {name}: ({explainer.alpha}, {explainer.beta}, {explainer.gamma})\")\n",
    "        else:\n",
    "            print(f\"({i}) {name}: ({explainer.alpha}, {explainer.beta}, X)\")\n",
    "    else:\n",
    "        print(f\"({i}) {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "191eee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12450"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(conta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
