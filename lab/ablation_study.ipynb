{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from utils.utils import *\n",
    "from src.dataset.instances.graph import GraphInstance\n",
    "from src.evaluation.evaluation_metric_runtime import RuntimeMetric\n",
    "from src.evaluation.evaluation_metric_correctness import CorrectnessMetric\n",
    "from src.evaluation.evaluation_metric_fidelity import FidelityMetric\n",
    "from src.evaluation.evaluation_metric_implausibility import ImplausibilityMetric\n",
    "from src.evaluation.evaluation_metric_dissimilarity import M_dissim_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_manager_path = \"..\\\\..\\\\explainability\\GRETEL-repo\\\\output\\\\eval_manager\\\\\"\n",
    "\n",
    "file_name = get_most_recent_file(eval_manager_path).split('.')[0]\n",
    "print(file_name)\n",
    "\n",
    "with open(eval_manager_path + file_name + '.pkl', 'rb') as f:\n",
    "    eval_manager = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oracle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oracle_metrics(eval_manager):\n",
    "    instances = eval_manager.evaluators[0].dataset.instances\n",
    "    oracle = eval_manager.evaluators[0]._oracle\n",
    "\n",
    "    grouped = {}\n",
    "    for inst in instances:\n",
    "        key = f\"{inst.patient_id}_{inst.record_id}\"\n",
    "        grouped.setdefault(key, []).append(inst)\n",
    "\n",
    "    rows = []\n",
    "    for pr, inst_list in grouped.items():\n",
    "        y_true = [i.label for i in inst_list]\n",
    "        y_pred = [oracle.predict(i) for i in inst_list]\n",
    "        rows.append({\n",
    "            \"patient_record\": pr,\n",
    "            \"accuracy\": round(accuracy_score(y_true, y_pred),4),\n",
    "            \"f1_score\": round(f1_score(y_true, y_pred),4),\n",
    "            \"recall\": round(recall_score(y_true, y_pred),4),\n",
    "            \"precision\": round(precision_score(y_true, y_pred),4)\n",
    "        })\n",
    "\n",
    "    y_true_all = [i.label for i in instances]\n",
    "    y_pred_all = [oracle.predict(i) for i in instances]\n",
    "    global_row = {\n",
    "        \"patient_record\": \"ALL\",\n",
    "        \"accuracy\": round(accuracy_score(y_true_all, y_pred_all),4),\n",
    "        \"f1_score\": round(f1_score(y_true_all, y_pred_all),4),\n",
    "        \"recall\": round(recall_score(y_true_all, y_pred_all),4),\n",
    "        \"precision\": round(precision_score(y_true_all, y_pred_all),4)\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([global_row] + rows)\n",
    "\n",
    "oracle_metrics = get_oracle_metrics(eval_manager)\n",
    "print(oracle_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top $k$ counterfactuals and explainer metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_keys = ['ID', 'Runtime', 'Correctness', 'Fidelity', 'Implausibility', 'Dissimilarity', 'Accuracy']\n",
    "metrics_keys = ['ID', 'Correctness', 'Fidelity', 'Implausibility', 'Dissimilarity', 'Accuracy']\n",
    "metrics = {k: {} for k in metrics_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, evaluator in enumerate(eval_manager._evaluators):\n",
    "    explainer = evaluator._explainer\n",
    "    name = explainer.name.split('-')[0]\n",
    "    if \"Temporal\" in name:\n",
    "        if 'NoStability' not in name:\n",
    "            print(f\"({i}) {name}: ({explainer.alpha}, {explainer.beta}, {explainer.gamma})\")\n",
    "        else:\n",
    "            print(f\"({i}) {name}: ({explainer.alpha}, {explainer.beta}, X)\")\n",
    "    else:\n",
    "        print(f\"({i}) {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(eval_manager._evaluators)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctness_metric = CorrectnessMetric()\n",
    "fidelity_metric = FidelityMetric()\n",
    "implausibility_metric = ImplausibilityMetric()\n",
    "dissimilarity_metric = M_dissim_metric()\n",
    "# runtime_metric = RuntimeMetric()\n",
    "\n",
    "all_cf_dicts = {}\n",
    "\n",
    "for i in indices:\n",
    "    # Initialize empty lists for each metric for this evaluator\n",
    "    for key in metrics_keys:\n",
    "        metrics[key][i] = []\n",
    "\n",
    "    cf_dict = {}\n",
    "\n",
    "    evaluator = eval_manager._evaluators[i]\n",
    "    oracle = evaluator._oracle\n",
    "    explainer = evaluator._explainer\n",
    "    dataset = evaluator.dataset.instances\n",
    "\n",
    "    # Select only correctly predicted positive instances\n",
    "    list_instances = [\n",
    "        inst for inst in dataset\n",
    "        if inst.label == 1 and inst.patient_id == \"chb03\" and inst.record_id == \"01\"\n",
    "    ]\n",
    "\n",
    "    # list_instances = list_instances[:10]\n",
    "\n",
    "    # Check if the explainer's explain method takes two arguments (i.e., has return_list=True)\n",
    "    has_return_list = len(inspect.signature(explainer.explain).parameters) == 2\n",
    "\n",
    "    for instance in tqdm(list_instances, desc=f\"Evaluator {i}\"):\n",
    "        # Generate counterfactual for the instance\n",
    "        result = explainer.explain(instance, return_list=True) if has_return_list else explainer.explain(instance)\n",
    "        cf_dict[instance.id] = result\n",
    "\n",
    "        # Extract the counterfactual instance\n",
    "        counterfactual = result if isinstance(result, GraphInstance) else result[0][1]\n",
    "\n",
    "        # Record info and metrics\n",
    "        metrics['ID'][i].append(instance.id)\n",
    "        # metrics['Runtime'][i].append(runtime_metric.evaluate(instance, counterfactual, oracle, explainer)[0])\n",
    "        metrics['Correctness'][i].append(correctness_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Fidelity'][i].append(fidelity_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Implausibility'][i].append(implausibility_metric.evaluate(instance, counterfactual, dataset=dataset))\n",
    "        metrics['Dissimilarity'][i].append(dissimilarity_metric.evaluate(instance, counterfactual, oracle, explainer))\n",
    "        metrics['Accuracy'][i].append(oracle.predict(instance) == 1)\n",
    "\n",
    "    all_cf_dicts[i] = cf_dict\n",
    "\n",
    "with open(f\"output/cf_dict/cf_dict_{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(all_cf_dicts, f)\n",
    "\n",
    "with open(f\"output/cf_dict/metrics_{file_name}.pkl\", 'wb') as f:\n",
    "    pickle.dump(metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainer metrics (all patients, only top counterfactual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "with open(f\"output/cf_dict/metrics_{file_name}.pkl\", 'rb') as f:\n",
    "    metrics = pickle.load(f)\n",
    "\n",
    "for eval_id in metrics['ID'].keys():\n",
    "    row = {\"Evaluator\": eval_id}\n",
    "    acc = np.array(metrics['Accuracy'][eval_id])\n",
    "\n",
    "    for metric in metrics_keys[1:]:\n",
    "        vals = np.array(metrics[metric][eval_id])\n",
    "\n",
    "        # Implausibility and Dissimilarity are only computed for well-predicted instances\n",
    "        if metric in ['Implausibility', 'Dissimilarity']:\n",
    "            filtered_vals = vals[acc == True]\n",
    "            mean_val = np.mean(filtered_vals) if filtered_vals.size > 0 else np.nan\n",
    "        else:\n",
    "            mean_val = np.mean(vals) if vals.size > 0 else np.nan\n",
    "        row[metric] = round(mean_val, 4) if not np.isnan(mean_val) else np.nan\n",
    "        \n",
    "    rows.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(rows).set_index(\"Evaluator\")\n",
    "print(summary_df.iloc[:,:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ablation study (single patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = \"chb03\"\n",
    "record_id = \"01\"\n",
    "\n",
    "with open(f\"output/cf_dict/cf_dict_{file_name}.pkl\", \"rb\") as f:\n",
    "    all_cf_dicts = pickle.load(f)\n",
    "\n",
    "accurate_ids = {\n",
    "    i: set(\n",
    "        id_ for id_, acc in zip(metrics['ID'][i], metrics['Accuracy'][i]) if acc\n",
    "    )\n",
    "    for i in metrics['ID'].keys()\n",
    "}\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, evaluator in enumerate(eval_manager._evaluators):\n",
    "    explainer = evaluator._explainer\n",
    "\n",
    "    if 'Temporal' not in explainer.name:\n",
    "        continue\n",
    "\n",
    "    cf_dict = all_cf_dicts[i]\n",
    "\n",
    "    m_d, m_t, m_i = [], [], []\n",
    "\n",
    "    for instance in evaluator.dataset.instances:\n",
    "        if instance.patient_id != patient_id or instance.record_id != record_id:\n",
    "            continue\n",
    "\n",
    "        if instance.id not in accurate_ids.get(i, set()):\n",
    "            continue\n",
    "\n",
    "        d_vals, t_vals, i_vals = [], [], []\n",
    "\n",
    "        for cf in cf_dict[instance.id]:\n",
    "            d, t, instab = explainer.compute_metric_components(instance, cf[1])\n",
    "            d_vals.append(d)\n",
    "            t_vals.append(np.sqrt(t))\n",
    "            i_vals.append(instab)\n",
    "\n",
    "        if d_vals:\n",
    "            m_d.append((np.mean(d_vals), np.std(d_vals)))\n",
    "            m_t.append((np.mean(t_vals), np.std(t_vals)))\n",
    "            m_i.append((np.mean(i_vals), np.std(i_vals)))\n",
    "\n",
    "    def format_metric(pairs):\n",
    "        if not pairs:\n",
    "            return \"nan ± nan\"\n",
    "        mean = np.mean([m for m, _ in pairs])\n",
    "        std = np.mean([s for _, s in pairs])\n",
    "        return f\"{mean:.4f} ± {std:.4f}\"\n",
    "    \n",
    "    if 'NoStability' not in explainer.name:\n",
    "        parameters = f\"({explainer.alpha}, {explainer.beta}, {explainer.gamma})\"\n",
    "    else:\n",
    "        parameters = f\"({explainer.alpha}, {explainer.beta}, X)\"\n",
    "\n",
    "    row = {\n",
    "        \"Parameters\": parameters,\n",
    "        \"M_dissim\": format_metric(m_d),\n",
    "        \"sqrt(M_time)\": format_metric(m_t),\n",
    "        \"M_instab\": format_metric(m_i)\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "if rows:\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    print(f\"Record: {patient_id}_{record_id}\\n\")\n",
    "    print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRETEL_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
